<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Interview Questions and Answers - Complete Guide 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .header {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            text-align: center;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .header h1 {
            font-size: 3em;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 10px;
            font-weight: 700;
        }

        .header p {
            font-size: 1.2em;
            color: #666;
            margin-bottom: 20px;
        }

        .stats {
            display: flex;
            justify-content: center;
            gap: 30px;
            flex-wrap: wrap;
        }

        .stat-item {
            text-align: center;
            padding: 15px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border-radius: 15px;
            min-width: 120px;
            box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
        }

        .stat-number {
            font-size: 2em;
            font-weight: bold;
            display: block;
        }

        .content-section {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .section-title {
            font-size: 2.5em;
            color: #333;
            margin-bottom: 30px;
            text-align: center;
            position: relative;
        }

        .section-title::after {
            content: '';
            position: absolute;
            bottom: -10px;
            left: 50%;
            transform: translateX(-50%);
            width: 100px;
            height: 4px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 2px;
        }

        .toc {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 40px;
            border-left: 5px solid #667eea;
        }

        .toc h3 {
            color: #333;
            margin-bottom: 20px;
            font-size: 1.5em;
        }

        .toc ul {
            list-style: none;
            columns: 2;
            column-gap: 30px;
        }

        .toc li {
            margin-bottom: 10px;
            break-inside: avoid;
        }

        .toc a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
            display: block;
            padding: 8px 0;
            border-bottom: 1px solid transparent;
        }

        .toc a:hover {
            color: #764ba2;
            border-bottom-color: #764ba2;
            transform: translateX(5px);
        }

        .question-container {
            margin-bottom: 30px;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
        }

        .question-container:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
        }

        .question {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 20px;
            cursor: pointer;
            font-weight: 600;
            font-size: 1.1em;
            display: flex;
            justify-content: space-between;
            align-items: center;
            transition: all 0.3s ease;
        }

        .question:hover {
            background: linear-gradient(135deg, #5a6fd8, #6a4190);
        }

        .question::after {
            content: '+';
            font-size: 1.5em;
            transition: transform 0.3s ease;
        }

        .question.active::after {
            transform: rotate(45deg);
        }

        .answer {
            background: white;
            padding: 0;
            max-height: 0;
            overflow: hidden;
            transition: all 0.3s ease;
        }

        .answer.active {
            padding: 25px;
            max-height: 1000px;
        }

        .answer-content {
            color: #444;
            line-height: 1.8;
        }

        .code-block {
            background: #1e1e1e;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Monaco', 'Consolas', monospace;
            box-shadow: inset 0 2px 10px rgba(0, 0, 0, 0.3);
        }

        .highlight {
            background: linear-gradient(120deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
            padding: 15px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
            margin: 15px 0;
        }

        .difficulty {
            display: inline-block;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
            margin-bottom: 10px;
        }

        .easy { background: #d4edda; color: #155724; }
        .medium { background: #fff3cd; color: #856404; }
        .hard { background: #f8d7da; color: #721c24; }

        .category-section {
            margin-bottom: 50px;
        }

        .category-title {
            font-size: 2em;
            color: #333;
            margin-bottom: 25px;
            padding-bottom: 10px;
            border-bottom: 2px solid #667eea;
            position: relative;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }

            .header h1 {
                font-size: 2em;
            }

            .stats {
                gap: 15px;
            }

            .toc ul {
                columns: 1;
            }

            .section-title {
                font-size: 2em;
            }
        }

        .intro-text {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 30px;
            text-align: center;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        .tips-section {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1), rgba(118, 75, 162, 0.1));
            border-radius: 15px;
            padding: 30px;
            margin-top: 40px;
        }

        .tips-section h3 {
            color: #333;
            margin-bottom: 20px;
            font-size: 1.5em;
        }

        .tips-section ul {
            list-style-type: none;
            padding: 0;
        }

        .tips-section li {
            padding: 10px 0;
            border-bottom: 1px solid rgba(102, 126, 234, 0.2);
            position: relative;
            padding-left: 30px;
        }

        .tips-section li::before {
            content: '‚úì';
            position: absolute;
            left: 0;
            color: #667eea;
            font-weight: bold;
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>PySpark Interview Questions & Answers</h1>
            <p>Master Apache Spark with Python - Complete Interview Preparation Guide 2025</p>
            <div class="stats">
                <div class="stat-item">
                    <span class="stat-number">50+</span>
                    <span>Questions</span>
                </div>
                <div class="stat-item">
                    <span class="stat-number">3</span>
                    <span>Difficulty Levels</span>
                </div>
                <div class="stat-item">
                    <span class="stat-number">8</span>
                    <span>Categories</span>
                </div>
            </div>
        </div>

        <div class="content-section">
            <div class="intro-text">
                This comprehensive guide covers the most important PySpark interview questions and answers. Whether you're a beginner or an experienced data engineer, these questions will help you prepare for your next PySpark interview and land your dream job in big data analytics.
            </div>

            <div class="toc">
                <h3>üìö Table of Contents</h3>
                <ul>
                    <li><a href="#basics">PySpark Basics</a></li>
                    <li><a href="#rdd">RDD Operations</a></li>
                    <li><a href="#dataframes">DataFrames & Datasets</a></li>
                    <li><a href="#sql">Spark SQL</a></li>
                    <li><a href="#streaming">Spark Streaming</a></li>
                    <li><a href="#mllib">MLlib & Machine Learning</a></li>
                    <li><a href="#performance">Performance Optimization</a></li>
                    <li><a href="#advanced">Advanced Topics</a></li>
                </ul>
            </div>
        </div>

        <!-- PySpark Basics -->
        <div class="content-section">
            <div class="category-section" id="basics">
                <h2 class="category-title">üöÄ PySpark Basics</h2>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty easy">Easy</span> What is PySpark and how does it differ from Apache Spark?</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <p><strong>PySpark</strong> is the Python API for Apache Spark, allowing developers to harness the power of Apache Spark using Python programming language.</p>
                            
                            <div class="highlight">
                                <strong>Key Differences:</strong><br>
                                ‚Ä¢ <strong>Language:</strong> PySpark uses Python while Apache Spark is written in Scala<br>
                                ‚Ä¢ <strong>Performance:</strong> Scala-based Spark is generally faster due to JVM optimization<br>
                                ‚Ä¢ <strong>Ease of Use:</strong> PySpark is more beginner-friendly due to Python's simplicity<br>
                                ‚Ä¢ <strong>Libraries:</strong> PySpark can leverage Python's extensive ecosystem (NumPy, Pandas, etc.)
                            </div>

                            <div class="code-block">
# Basic PySpark initialization
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApplication") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Create a simple DataFrame
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])
df.show()
                            </div>
                        </div>
                    </div>
                </div>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty easy">Easy</span> Explain the architecture of Apache Spark and PySpark's role in it.</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <p>Apache Spark follows a <strong>master-slave architecture</strong> with the following components:</p>
                            
                            <div class="highlight">
                                <strong>Core Components:</strong><br>
                                ‚Ä¢ <strong>Driver Program:</strong> Contains the main() function and SparkContext<br>
                                ‚Ä¢ <strong>Cluster Manager:</strong> Manages resources (YARN, Mesos, Standalone)<br>
                                ‚Ä¢ <strong>Worker Nodes:</strong> Execute tasks and store data<br>
                                ‚Ä¢ <strong>Executors:</strong> Run on worker nodes to execute tasks<br>
                                ‚Ä¢ <strong>Tasks:</strong> Units of work sent to executors
                            </div>

                            <p><strong>PySpark's Role:</strong> PySpark acts as a bridge between Python code and the Spark execution engine. It uses Py4J to communicate with the JVM where Spark runs.</p>

                            <div class="code-block">
# SparkContext and SparkSession
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

# Creating SparkContext (older approach)
conf = SparkConf().setAppName("MyApp").setMaster("local[*]")
sc = SparkContext(conf=conf)

# Creating SparkSession (recommended approach)
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .getOrCreate()
                            </div>
                        </div>
                    </div>
                </div>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty medium">Medium</span> What are the key features and benefits of using PySpark?</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <div class="highlight">
                                <strong>Key Features:</strong><br>
                                ‚Ä¢ <strong>In-Memory Computing:</strong> 100x faster than Hadoop MapReduce<br>
                                ‚Ä¢ <strong>Fault Tolerance:</strong> Automatic recovery from node failures<br>
                                ‚Ä¢ <strong>Lazy Evaluation:</strong> Optimizes execution plans<br>
                                ‚Ä¢ <strong>Multi-language Support:</strong> Python, Scala, Java, R, SQL<br>
                                ‚Ä¢ <strong>Rich APIs:</strong> High-level APIs for complex operations
                            </div>

                            <div class="highlight">
                                <strong>Benefits:</strong><br>
                                ‚Ä¢ <strong>Speed:</strong> In-memory processing for iterative algorithms<br>
                                ‚Ä¢ <strong>Scalability:</strong> Handles petabytes of data across clusters<br>
                                ‚Ä¢ <strong>Versatility:</strong> Batch processing, streaming, ML, graph processing<br>
                                ‚Ä¢ <strong>Python Integration:</strong> Leverage existing Python libraries<br>
                                ‚Ä¢ <strong>Cost-Effective:</strong> Efficient resource utilization
                            </div>

                            <div class="code-block">
# Example: Processing large dataset efficiently
from pyspark.sql import functions as F

# Read large CSV file
df = spark.read.option("header", "true").csv("large_dataset.csv")

# Lazy evaluation - no execution yet
filtered_df = df.filter(F.col("amount") > 1000) \
               .groupBy("category") \
               .agg(F.sum("amount").alias("total_amount"))

# Action triggers execution
result = filtered_df.collect()  # Now execution happens
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- RDD Operations -->
        <div class="content-section">
            <div class="category-section" id="rdd">
                <h2 class="category-title">üîß RDD Operations</h2>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty easy">Easy</span> What is an RDD and what are its key characteristics?</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <p><strong>RDD (Resilient Distributed Dataset)</strong> is the fundamental data structure of Apache Spark. It's an immutable, distributed collection of objects.</p>

                            <div class="highlight">
                                <strong>Key Characteristics:</strong><br>
                                ‚Ä¢ <strong>Resilient:</strong> Fault-tolerant through lineage information<br>
                                ‚Ä¢ <strong>Distributed:</strong> Data is distributed across cluster nodes<br>
                                ‚Ä¢ <strong>Immutable:</strong> Cannot be changed after creation<br>
                                ‚Ä¢ <strong>Lazy Evaluation:</strong> Computed only when actions are called<br>
                                ‚Ä¢ <strong>In-Memory:</strong> Can be cached in memory for faster access
                            </div>

                            <div class="code-block">
# Creating RDDs
from pyspark import SparkContext

sc = SparkContext("local[*]", "RDD Example")

# From Python collection
rdd1 = sc.parallelize([1, 2, 3, 4, 5])

# From external file
rdd2 = sc.textFile("hdfs://path/to/file.txt")

# From another RDD
rdd3 = rdd1.map(lambda x: x * 2)

print(rdd1.collect())  # [1, 2, 3, 4, 5]
print(rdd3.collect())  # [2, 4, 6, 8, 10]
                            </div>
                        </div>
                    </div>
                </div>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty medium">Medium</span> Explain the difference between transformations and actions in RDD operations.</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <div class="highlight">
                                <strong>Transformations:</strong><br>
                                ‚Ä¢ Create a new RDD from existing RDD<br>
                                ‚Ä¢ Lazy evaluation - not executed immediately<br>
                                ‚Ä¢ Examples: map(), filter(), flatMap(), union(), distinct()
                            </div>

                            <div class="highlight">
                                <strong>Actions:</strong><br>
                                ‚Ä¢ Return values to driver program or write to storage<br>
                                ‚Ä¢ Trigger execution of transformations<br>
                                ‚Ä¢ Examples: collect(), count(), first(), take(), saveAsTextFile()
                            </div>

                            <div class="code-block">
# Transformations (lazy evaluation)
rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# These are transformations - no execution yet
even_rdd = rdd.filter(lambda x: x % 2 == 0)
squared_rdd = even_rdd.map(lambda x: x ** 2)
distinct_rdd = squared_rdd.distinct()

# Actions - trigger execution
result = distinct_rdd.collect()  # [4, 16, 36, 64, 100]
count = distinct_rdd.count()     # 5
first_element = distinct_rdd.first()  # 4

# Wide vs Narrow Transformations
# Narrow: map, filter (no shuffling)
narrow = rdd.map(lambda x: x * 2)

# Wide: groupByKey, reduceByKey (requires shuffling)
pairs = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
wide = pairs.groupByKey()
                            </div>
                        </div>
                    </div>
                </div>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty hard">Hard</span> How does RDD lineage work and why is it important for fault tolerance?</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <p><strong>RDD Lineage</strong> is a graph of dependencies between RDDs that Spark uses to track how each RDD was computed from its parent RDDs.</p>

                            <div class="highlight">
                                <strong>How Lineage Works:</strong><br>
                                ‚Ä¢ Each RDD maintains a pointer to its parent RDD(s)<br>
                                ‚Ä¢ Forms a Directed Acyclic Graph (DAG)<br>
                                ‚Ä¢ Stores the transformation function used to create the RDD<br>
                                ‚Ä¢ Enables lazy evaluation and optimization
                            </div>

                            <div class="highlight">
                                <strong>Fault Tolerance Benefits:</strong><br>
                                ‚Ä¢ <strong>Automatic Recovery:</strong> Recompute lost partitions using lineage<br>
                                ‚Ä¢ <strong>No Replication Overhead:</strong> No need to replicate data<br>
                                ‚Ä¢ <strong>Efficient Recovery:</strong> Only recompute affected partitions<br>
                                ‚Ä¢ <strong>Lineage Truncation:</strong> Checkpointing breaks long lineage chains
                            </div>

                            <div class="code-block">
# Example demonstrating lineage
rdd1 = sc.parallelize([1, 2, 3, 4, 5])
rdd2 = rdd1.map(lambda x: x * 2)
rdd3 = rdd2.filter(lambda x: x > 5)
rdd4 = rdd3.map(lambda x: x + 1)

# Check lineage
print(rdd4.toDebugString())
# Output shows the lineage chain:
# (4) PythonRDD[3] at RDD at PythonRDD.scala:53 []
#  |  MapPartitionsRDD[2] at map at NativeMethodAccessorImpl.java:0 []
#  |  MapPartitionsRDD[1] at map at NativeMethodAccessorImpl.java:0 []
#  |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []

# Persistence and checkpointing
rdd4.persist()  # Cache in memory
rdd4.checkpoint()  # Break lineage chain

# If a partition is lost, Spark can recompute it
# using the stored lineage information
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- DataFrames & Datasets -->
        <div class="content-section">
            <div class="category-section" id="dataframes">
                <h2 class="category-title">üìä DataFrames & Datasets</h2>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty easy">Easy</span> What are DataFrames in PySpark and how do they differ from RDDs?</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <p><strong>DataFrames</strong> are distributed collections of data organized into named columns, similar to tables in relational databases or data frames in R/Pandas.</p>

                            <div class="highlight">
                                <strong>DataFrame vs RDD:</strong><br>
                                ‚Ä¢ <strong>Schema:</strong> DataFrames have defined schema, RDDs don't<br>
                                ‚Ä¢ <strong>Optimization:</strong> DataFrames use Catalyst optimizer<br>
                                ‚Ä¢ <strong>API:</strong> Higher-level API with SQL-like operations<br>
                                ‚Ä¢ <strong>Performance:</strong> Better performance due to optimization<br>
                                ‚Ä¢ <strong>Language Integration:</strong> Consistent across Scala, Java, Python, R
                            </div>

                            <div class="code-block">
# Creating DataFrames
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()

# From Python list
data = [("Alice", 25, "Engineer"), ("Bob", 30, "Manager"), ("Charlie", 35, "Analyst")]
df1 = spark.createDataFrame(data, ["name", "age", "job"])

# With explicit schema
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("job", StringType(), True)
])
df2 = spark.createDataFrame(data, schema)

# From file
df3 = spark.read.csv("employees.csv", header=True, inferSchema=True)

# Basic operations
df1.show()
df1.printSchema()
df1.describe().show()
                            </div>
                        </div>
                    </div>
                </div>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty medium">Medium</span> Explain common DataFrame operations and transformations.</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <div class="code-block">
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Sample DataFrame
df = spark.createDataFrame([
    ("Alice", 25, "Engineering", 75000),
    ("Bob", 30, "Marketing", 65000),
    ("Charlie", 35, "Engineering", 85000),
    ("Diana", 28, "Marketing", 70000)
], ["name", "age", "department", "salary"])

# Selection and filtering
df.select("name", "salary").show()
df.filter(F.col("age") > 27).show()
df.where(df.salary > 70000).show()

# Aggregations
df.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.count("*").alias("employee_count")
).show()

# Window functions
window_spec = Window.partitionBy("department").orderBy("salary")
df.withColumn("rank", F.row_number().over(window_spec)).show()

# Joins
dept_df = spark.createDataFrame([
    ("Engineering", "Tech"),
    ("Marketing", "Business")
], ["department", "category"])

result = df.join(dept_df, "department", "inner").show()

# Column operations
df.withColumn("salary_k", F.col("salary") / 1000) \
  .withColumn("age_group", 
    F.when(F.col("age") < 30, "Young")
     .when(F.col("age") < 35, "Mid")
     .otherwise("Senior")) \
  .show()
                            </div>
                        </div>
                    </div>
                </div>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty medium">Medium</span> What is the Catalyst Optimizer and how does it improve DataFrame performance?</span>
                    </div>
                    <div class="answer">
                        <div class="answer-content">
                            <p><strong>Catalyst Optimizer</strong> is Spark's query optimization framework that automatically optimizes DataFrame and SQL queries.</p>

                            <div class="highlight">
                                <strong>Optimization Phases:</strong><br>
                                ‚Ä¢ <strong>Logical Plan:</strong> Parse and create initial logical plan<br>
                                ‚Ä¢ <strong>Logical Optimization:</strong> Apply rule-based optimizations<br>
                                ‚Ä¢ <strong>Physical Planning:</strong> Generate physical execution plans<br>
                                ‚Ä¢ <strong>Code Generation:</strong> Generate Java bytecode for execution
                            </div>

                            <div class="highlight">
                                <strong>Key Optimizations:</strong><br>
                                ‚Ä¢ <strong>Predicate Pushdown:</strong> Move filters closer to data source<br>
                                ‚Ä¢ <strong>Projection Pruning:</strong> Only read required columns<br>
                                ‚Ä¢ <strong>Constant Folding:</strong> Evaluate constants at compile time<br>
                                ‚Ä¢ <strong>Join Reordering:</strong> Optimize join order<br>
                                ‚Ä¢ <strong>Whole-Stage Code Generation:</strong> Generate efficient code
                            </div>

                            <div class="code-block">
# Example showing Catalyst optimization
df = spark.read.parquet("large_dataset.parquet")

# Original query
result = df.filter(F.col("status") == "active") \
           .select("id", "name", "amount") \
           .filter(F.col("amount") > 1000) \
           .groupBy("name") \
           .sum("amount")

# Catalyst automatically optimizes to:
# 1. Push filters down to data source
# 2. Only read required columns (projection pruning)
# 3. Combine filters
# 4. Optimize aggregation

# View execution plan
result.explain(True)  # Shows logical and physical plans

# Enable cost-based optimization
spark.conf.set("spark.sql.cbo.enabled", "true")
spark.conf.set("spark.sql.adaptive.enabled", "true")
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Spark SQL -->
        <div class="content-section">
            <div class="category-section" id="sql">
                <h2 class="category-title">üóÉÔ∏è Spark SQL</h2>

                <div class="question-container">
                    <div class="question">
                        <span><span class="difficulty easy">Easy</span
